{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcipvu5rJCn72yiraN5wNb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine Learning Assignment\n","Στο συγγεκριμένο notebook βρίσκονται οι ασκήσεις του μαθήματος \"Μηχανική Μάθηση\" της χρονιάς 2024-2025.\n","\n","Το dataset που χρησιμοποίησα είναι το Wine Quality Dataset του πανεπιστημίου UC Irvine (https://archive.ics.uci.edu/dataset/186/wine+quality). Πρόκειται για ένα dataset με 11 features και 1 label, την ποιότητα του κρασιού, που είναι ένα σκορ από 0 εώς 10. Για τους σκοπούς της άσκησης, θα υποθέσουμε ότι το κάθε σκορ μπορεί να ανήκει σε μία από 5 κατηγορίες:\n","\n","*   Κάκιστο (Very Bad. Score: 0-2)\n","*   Κακό (Bad. Score: 2-4)\n","*   Εντάξει (Alright. Score 4-6)\n","*   Καλό (Good. Score 6-8)\n","*   Άριστο (Excellent. Score 8-10)\n","\n","Τα ερωτήματα είναι τεράστια για ένα notebook, οπότε αποφάσισα να τα χωρίσω ανά 2. Το συγκεκριμένο notebook θα έχει τα παρακάτω ερωτήματα:\n","\n","7.   Support Vector Machine (SVM)\n","8.   K Means"],"metadata":{"id":"lffWBPnTlWeg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xx0cP4q1iu0p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736779184967,"user_tz":-120,"elapsed":28239,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"1040e886-0f0a-4e3f-8c5e-3016ca8bcc68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/MLAssignment/Data')"]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"NB9vA_733ng_"}},{"cell_type":"code","source":["wines = pd.read_csv('winequality-red.csv', sep=';')\n","\n","features = wines.drop(\"quality\", axis=1)\n","label = wines[\"quality\"]\n","\n","X = np.array(features)\n","y = wines[[\"quality\"]].to_numpy() #For shape (n,1)\n","\n","# I am aware this method is not good for edge cases, but it is what it is\n","\n","# Changed the classification from string to int in order do do one-hot vectors easier later\n","\n","def classify(y):\n","    if y < 2.0:\n","        y = 0\n","    elif y < 4.0:\n","        y = 1\n","    elif y < 6.0:\n","        y = 2\n","    elif y < 8.0:\n","        y = 3\n","    else:\n","        y = 4\n","    return y\n","\n","num_classes = 5\n","y_classified = np.array([classify(label) for label in y])\n","\n","y_one_hot = np.eye(y_classified.max() + 1)[y_classified] #transform the label to a one-hot vector"],"metadata":{"id":"-ZE7bSin3nyd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_features(X):\n","    mean = np.mean(X, axis=0)\n","    std = np.std(X, axis=0)\n","    X_normalized = (X - mean) / std\n","    return X_normalized\n","\n","X_normalized = normalize_features(X)"],"metadata":{"id":"-IWfA1iS38M1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7) Support Vector Machine (SVM)"],"metadata":{"id":"51sLFIer39oW"}},{"cell_type":"code","source":["class SVM:\n","    def __init__(self, learning_rate=0.001, _l=0.01, n_iters=1000, class_pred=3):\n","        self.lr = learning_rate\n","        self._l = _l\n","        self.n_iters = n_iters\n","        self.class_pred = class_pred\n","        self.w = None\n","        self.b = None\n","\n","    def fit(self, X, y_classified): # specified y_classified since it won't work with normal y or one_hot y\n","        n_samples, n_features = X.shape\n","        _y = np.where(y_classified != self.class_pred, -1, 1)\n","\n","        self.w = np.zeros(n_features)\n","        self.b = 0\n","\n","        for _ in range(self.n_iters):\n","            for idx, x_i in enumerate(X):\n","                condition = _y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n","                if condition:\n","                    self.w -= self.lr * (2 * self._l * self.w)\n","                else:\n","                    self.w -= self.lr * (2 * self._l * self.w - np.dot(x_i, _y[idx]))\n","                    self.b -= self.lr * _y[idx]\n","\n","    def predict (self, X):\n","        approx = np.dot(X, self.w) - self.b\n","        return np.sign(approx)"],"metadata":{"id":"7ViKMR3M4BeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_classified, test_size=0.2, random_state=42)"],"metadata":{"id":"ys3KRz_TGerY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# REPLACE CLASS TEST WITH THE CLASS YOU WANT TO PREDICT\n","\n","class_test = 3\n","test = SVM(class_pred=class_test)"],"metadata":{"id":"DyTTQ4gFNVN1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.fit(X_train, y_train)"],"metadata":{"id":"lK5cXKHsG7RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["true_positives = np.sum(np.where(y_test == 3, 1, 0), dtype=np.float32)\n","test_positives = np.sum(np.where(test.predict(X_test) == 1, 1, 0))\n","print (f\"Accuracy = {test_positives / true_positives}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyA3itcuLNFt","executionInfo":{"status":"ok","timestamp":1736781311277,"user_tz":-120,"elapsed":261,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"5dcb868c-68a0-4666-bee2-3fd2c27dec1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy = 0.8908045977011494\n"]}]},{"cell_type":"markdown","source":["# 8) K-Means"],"metadata":{"id":"7t5vyUGLOb1E"}},{"cell_type":"code","source":["def euclidean_distance(x1, x2):\n","    return np.sqrt(np.sum((x1-x2)**2))\n","\n","class KMeans:\n","\n","    def __init__(self, K=5, max_iters=100, plot_steps=False):\n","        self.K = K\n","        self.max_iters = max_iters\n","        self.plot_steps = plot_steps\n","\n","        # list of sample indices for each cluster\n","        self.clusters = [[] for _ in range(self.K)]\n","\n","        # the centers (mean vector) for each cluster\n","        self.centroids = []\n","\n","\n","    def predict(self, X):\n","        self.X = X\n","        self.n_samples, self.n_features = X.shape\n","\n","        # initialize\n","        random_sample_idxs = np.random.choice(self.n_samples, self.K, replace=False)\n","        self.centroids = [self.X[idx] for idx in random_sample_idxs]\n","\n","        # optimize clusters\n","        for _ in range(self.max_iters):\n","            # assign samples to closest centroids (create clusters)\n","            self.clusters = self._create_clusters(self.centroids)\n","\n","            if self.plot_steps:\n","                self.plot()\n","\n","            # calculate new centroids from the clusters\n","            centroids_old = self.centroids\n","            self.centroids = self._get_centroids(self.clusters)\n","\n","            if self._is_converged(centroids_old, self.centroids):\n","                break\n","\n","            if self.plot_steps:\n","                self.plot()\n","\n","        # classify samples as the index of their clusters\n","        return self._get_cluster_labels(self.clusters)\n","\n","    def _get_cluster_labels(self, clusters):\n","        # each sample will get the label of the cluster it was assigned to\n","        labels = np.empty(self.n_samples)\n","        for cluster_idx, cluster in enumerate(clusters):\n","            for sample_idx in cluster:\n","                labels[sample_idx] = cluster_idx\n","\n","        return labels\n","\n","\n","    def _create_clusters(self, centroids):\n","        # assign the samples to the closest centroids\n","        clusters = [[] for _ in range(self.K)]\n","        for idx, sample in enumerate(self.X):\n","            centroid_idx = self._closest_centroid(sample, centroids)\n","            clusters[centroid_idx].append(idx)\n","        return clusters\n","\n","    def _closest_centroid(self, sample, centroids):\n","        # distance of the current sample to each centroid\n","        distances = [euclidean_distance(sample, point) for point in centroids]\n","        closest_idx = np.argmin(distances)\n","        return closest_idx\n","\n","\n","    def _get_centroids(self, clusters):\n","        # assign mean value of clusters to centroids\n","        centroids = np.zeros((self.K, self.n_features))\n","        for cluster_idx, cluster in enumerate(clusters):\n","            cluster_mean = np.mean(self.X[cluster], axis=0)\n","            centroids[cluster_idx] = cluster_mean\n","        return centroids\n","\n","    def _is_converged(self, centroids_old, centroids):\n","        # distances between old and new centroids, for all centroids\n","        distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]\n","        return sum(distances) == 0"],"metadata":{"id":"TF2MWrE4OfCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_classified, test_size=0.2, random_state=42)"],"metadata":{"id":"nFuJ9yHfQydR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = KMeans()\n","test.predict(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iznXEXw4RBKH","executionInfo":{"status":"ok","timestamp":1736781908156,"user_tz":-120,"elapsed":4846,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"f9c27ef6-4238-41a3-8493-c9fb4f7466dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4., 2., 2., ..., 4., 4., 4.])"]},"metadata":{},"execution_count":41}]}]}