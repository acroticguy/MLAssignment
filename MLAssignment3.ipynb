{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmKuvc4X8Z4rU16t8dCBM3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine Learning Assignment\n","Στο συγγεκριμένο notebook βρίσκονται οι ασκήσεις του μαθήματος \"Μηχανική Μάθηση\" της χρονιάς 2024-2025.\n","\n","Το dataset που χρησιμοποίησα είναι το Wine Quality Dataset του πανεπιστημίου UC Irvine (https://archive.ics.uci.edu/dataset/186/wine+quality). Πρόκειται για ένα dataset με 11 features και 1 label, την ποιότητα του κρασιού, που είναι ένα σκορ από 0 εώς 10. Για τους σκοπούς της άσκησης, θα υποθέσουμε ότι το κάθε σκορ μπορεί να ανήκει σε μία από 5 κατηγορίες:\n","\n","*   Κάκιστο (Very Bad. Score: 0-2)\n","*   Κακό (Bad. Score: 2-4)\n","*   Εντάξει (Alright. Score 4-6)\n","*   Καλό (Good. Score 6-8)\n","*   Άριστο (Excellent. Score 8-10)\n","\n","Τα ερωτήματα είναι τεράστια για ένα notebook, οπότε αποφάσισα να τα χωρίσω ανά 2. Το συγκεκριμένο notebook θα έχει τα παρακάτω ερωτήματα:\n","\n","5.   Naive Bayes\n","6.   Multilayer Perceptron (MLP) in Pytorch\n","\n","Στο τελευταίο αρχείο θα λυθούν τα θέματα με την ακόλουθη σειρά:\n","\n","7.   Support Vector Machine (SVM)\n","8.   K Means"],"metadata":{"id":"QXc7WRkqzYzU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6W_NFpD0zUhr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736429720935,"user_tz":-120,"elapsed":9087,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"c3bd93fc-da73-46d6-dee9-18b7b43f712a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/MLAssignment/Data')"]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"Tz1Zx8y8zn42"}},{"cell_type":"code","source":["wines = pd.read_csv('winequality-red.csv', sep=';')\n","\n","features = wines.drop(\"quality\", axis=1)\n","label = wines[\"quality\"]\n","\n","X = np.array(features)\n","y = wines[[\"quality\"]].to_numpy() #For shape (n,1)\n","\n","# I am aware this method is not good for edge cases, but it is what it is\n","\n","# Changed the classification from string to int in order do do one-hot vectors easier later\n","\n","def classify(y):\n","    if y < 2.0:\n","        y = 0\n","    elif y < 4.0:\n","        y = 1\n","    elif y < 6.0:\n","        y = 2\n","    elif y < 8.0:\n","        y = 3\n","    else:\n","        y = 4\n","    return y\n","\n","num_classes = 5\n","y_classified = np.array([classify(label) for label in y])\n","\n","y_one_hot = np.eye(y_classified.max() + 1)[y_classified] #transform the label to a one-hot vector"],"metadata":{"id":"-C6GM6LIzohf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_features(X):\n","    mean = np.mean(X, axis=0)\n","    std = np.std(X, axis=0)\n","    X_normalized = (X - mean) / std\n","    return X_normalized\n","\n","X_normalized = normalize_features(X)"],"metadata":{"id":"FVikrbMO0Y-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5) Naive Bayes Classifier"],"metadata":{"id":"1j6RWILs1h4u"}},{"cell_type":"code","source":["def calculate_mean_std_per_class(X, y):\n","    \"\"\"Calculates mean and std of features for each class.\"\"\"\n","    classes = np.unique(y)\n","    means = {}\n","    stds = {}\n","    for c in classes:\n","        X_c = X[y == c]  # Filter data for class c\n","        means[c] = np.mean(X_c, axis=0) # Mean of each feature for class c\n","        stds[c] = np.std(X_c, axis=0) # Std of each feature for class c\n","    return means, stds"],"metadata":{"id":"tjgrZPba1hGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gaussian_log_pdf(x, mean, std):\n","    \"\"\"Calculates the log of the probability density function for a Gaussian.\"\"\"\n","    epsilon = 1e-8 # Small value\n","    std = std + epsilon # Adding a small number to avoid std = 0\n","    exponent = -((x - mean) ** 2) / (2 * std**2)\n","    log_coefficient = -0.5 * np.log(2 * np.pi) - np.log(std)\n","    return log_coefficient + exponent"],"metadata":{"id":"OFIEOIyj1lqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_class_priors(y):\n","    \"\"\"Calculates prior probability for each class\"\"\"\n","    classes = np.unique(y)\n","    priors = {}\n","    for c in classes:\n","        priors[c] = np.sum(y == c) / len(y) # Count occurences of each class\n","    return priors"],"metadata":{"id":"azoVWBTY1o6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ml_naive_bayes_train(X, y):\n","    \"\"\"Trains the Naive Bayes classifier.\"\"\"\n","    means, stds = calculate_mean_std_per_class(X, y)\n","    priors = calculate_class_priors(y)\n","    return means, stds, priors"],"metadata":{"id":"w-IeCHUJ1svV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ml_naive_bayes_test(means, stds, priors, X_test):\n","    \"\"\"Applies the Naive Bayes classifier to the test data.\"\"\"\n","    classes = list(priors.keys())\n","    num_samples = X_test.shape[0]\n","    class_log_probs = np.zeros((num_samples, len(classes)))\n","    for idx, c in enumerate(classes):\n","        log_likelihood = gaussian_log_pdf(X_test, means[c], stds[c])\n","        class_log_probs[:, idx] = np.sum(log_likelihood, axis=1) + np.log(priors[c])\n","    y_test = np.argmax(class_log_probs, axis=1)\n","    return y_test"],"metadata":{"id":"JdrUR5ji1049"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train and Predict\n","means, stds, priors = ml_naive_bayes_train(X_normalized, y_classified) #Train\n","y_pred = ml_naive_bayes_test(means, stds, priors, X_normalized) #Test on training data\n","\n","#Calculate Accuracy\n","accuracy = np.mean(y_pred == y_classified) # Calculate acc on the training set\n","print(f\"Accuracy: {accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRSI6acw18EV","executionInfo":{"status":"ok","timestamp":1736418791527,"user_tz":-120,"elapsed":238,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"583ff8ca-85c8-488f-83dd-2ecb2788e24e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.13758599124452783\n"]}]},{"cell_type":"markdown","source":["# 6) MLP"],"metadata":{"id":"iYAYzGPc--mp"}},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plO-HgLRL6Dz","executionInfo":{"status":"ok","timestamp":1736429797876,"user_tz":-120,"elapsed":4744,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"c6e4ecd4-3ed2-457b-8d8c-8a6edd3b4498"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n","Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.1\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","from torchmetrics import Accuracy\n","torch.manual_seed(42) # Setting the seed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3SfCKjQ--E5","executionInfo":{"status":"ok","timestamp":1736429827103,"user_tz":-120,"elapsed":26358,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"7df83261-489a-4f71-9e9b-e47364ff83a4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f573954e2d0>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_classified, test_size=0.3, random_state=42)\n","\n","# Convert data to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n","\n","# Create TensorDatasets and DataLoaders for batching\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"ze92tc0_IBKu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the MLP model\n","class MLP(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(MLP, self).__init__()\n","        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n","        self.tanh_act = torch.nn.Tanh()\n","        self.linear2 = torch.nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.tanh_act(x)\n","        x = self.linear2(x)\n","        return x"],"metadata":{"id":"P5WaYqVrGsY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model parameters\n","input_size = X_normalized.shape[1] # Number of features\n","hidden_size = 128\n","\n","# Initialize the model, loss function, and optimizer\n","model = MLP(input_size, hidden_size, num_classes)\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.005) # You can tune learning rate, momentum, and other optimizer parameters"],"metadata":{"id":"Z9H2BU7HHUzp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Loop\n","num_epochs = 500\n","for epoch in range(num_epochs):\n","    model.train() # Set the model to training mode\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Calculate the loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimization step\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yvbq6HGoH0qu","executionInfo":{"status":"ok","timestamp":1736431294201,"user_tz":-120,"elapsed":20507,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"231c1a1b-4ff0-4ab3-a6a2-b1c97573e612"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/500, Loss: 1.5230\n","Epoch 2/500, Loss: 1.3222\n","Epoch 3/500, Loss: 1.1711\n","Epoch 4/500, Loss: 1.0589\n","Epoch 5/500, Loss: 0.9754\n","Epoch 6/500, Loss: 0.9130\n","Epoch 7/500, Loss: 0.8656\n","Epoch 8/500, Loss: 0.8289\n","Epoch 9/500, Loss: 0.7997\n","Epoch 10/500, Loss: 0.7761\n","Epoch 11/500, Loss: 0.7568\n","Epoch 12/500, Loss: 0.7406\n","Epoch 13/500, Loss: 0.7268\n","Epoch 14/500, Loss: 0.7153\n","Epoch 15/500, Loss: 0.7051\n","Epoch 16/500, Loss: 0.6961\n","Epoch 17/500, Loss: 0.6883\n","Epoch 18/500, Loss: 0.6816\n","Epoch 19/500, Loss: 0.6758\n","Epoch 20/500, Loss: 0.6699\n","Epoch 21/500, Loss: 0.6651\n","Epoch 22/500, Loss: 0.6605\n","Epoch 23/500, Loss: 0.6566\n","Epoch 24/500, Loss: 0.6527\n","Epoch 25/500, Loss: 0.6496\n","Epoch 26/500, Loss: 0.6464\n","Epoch 27/500, Loss: 0.6434\n","Epoch 28/500, Loss: 0.6405\n","Epoch 29/500, Loss: 0.6385\n","Epoch 30/500, Loss: 0.6361\n","Epoch 31/500, Loss: 0.6339\n","Epoch 32/500, Loss: 0.6318\n","Epoch 33/500, Loss: 0.6298\n","Epoch 34/500, Loss: 0.6282\n","Epoch 35/500, Loss: 0.6264\n","Epoch 36/500, Loss: 0.6249\n","Epoch 37/500, Loss: 0.6235\n","Epoch 38/500, Loss: 0.6223\n","Epoch 39/500, Loss: 0.6210\n","Epoch 40/500, Loss: 0.6197\n","Epoch 41/500, Loss: 0.6186\n","Epoch 42/500, Loss: 0.6174\n","Epoch 43/500, Loss: 0.6162\n","Epoch 44/500, Loss: 0.6154\n","Epoch 45/500, Loss: 0.6144\n","Epoch 46/500, Loss: 0.6134\n","Epoch 47/500, Loss: 0.6126\n","Epoch 48/500, Loss: 0.6119\n","Epoch 49/500, Loss: 0.6108\n","Epoch 50/500, Loss: 0.6101\n","Epoch 51/500, Loss: 0.6094\n","Epoch 52/500, Loss: 0.6086\n","Epoch 53/500, Loss: 0.6079\n","Epoch 54/500, Loss: 0.6072\n","Epoch 55/500, Loss: 0.6064\n","Epoch 56/500, Loss: 0.6059\n","Epoch 57/500, Loss: 0.6056\n","Epoch 58/500, Loss: 0.6049\n","Epoch 59/500, Loss: 0.6041\n","Epoch 60/500, Loss: 0.6034\n","Epoch 61/500, Loss: 0.6030\n","Epoch 62/500, Loss: 0.6026\n","Epoch 63/500, Loss: 0.6019\n","Epoch 64/500, Loss: 0.6016\n","Epoch 65/500, Loss: 0.6011\n","Epoch 66/500, Loss: 0.6007\n","Epoch 67/500, Loss: 0.6000\n","Epoch 68/500, Loss: 0.5997\n","Epoch 69/500, Loss: 0.5992\n","Epoch 70/500, Loss: 0.5987\n","Epoch 71/500, Loss: 0.5982\n","Epoch 72/500, Loss: 0.5978\n","Epoch 73/500, Loss: 0.5976\n","Epoch 74/500, Loss: 0.5972\n","Epoch 75/500, Loss: 0.5966\n","Epoch 76/500, Loss: 0.5963\n","Epoch 77/500, Loss: 0.5960\n","Epoch 78/500, Loss: 0.5954\n","Epoch 79/500, Loss: 0.5952\n","Epoch 80/500, Loss: 0.5950\n","Epoch 81/500, Loss: 0.5945\n","Epoch 82/500, Loss: 0.5942\n","Epoch 83/500, Loss: 0.5936\n","Epoch 84/500, Loss: 0.5932\n","Epoch 85/500, Loss: 0.5932\n","Epoch 86/500, Loss: 0.5925\n","Epoch 87/500, Loss: 0.5924\n","Epoch 88/500, Loss: 0.5920\n","Epoch 89/500, Loss: 0.5916\n","Epoch 90/500, Loss: 0.5915\n","Epoch 91/500, Loss: 0.5910\n","Epoch 92/500, Loss: 0.5909\n","Epoch 93/500, Loss: 0.5904\n","Epoch 94/500, Loss: 0.5903\n","Epoch 95/500, Loss: 0.5900\n","Epoch 96/500, Loss: 0.5897\n","Epoch 97/500, Loss: 0.5895\n","Epoch 98/500, Loss: 0.5892\n","Epoch 99/500, Loss: 0.5891\n","Epoch 100/500, Loss: 0.5885\n","Epoch 101/500, Loss: 0.5883\n","Epoch 102/500, Loss: 0.5880\n","Epoch 103/500, Loss: 0.5879\n","Epoch 104/500, Loss: 0.5874\n","Epoch 105/500, Loss: 0.5872\n","Epoch 106/500, Loss: 0.5873\n","Epoch 107/500, Loss: 0.5870\n","Epoch 108/500, Loss: 0.5866\n","Epoch 109/500, Loss: 0.5862\n","Epoch 110/500, Loss: 0.5861\n","Epoch 111/500, Loss: 0.5858\n","Epoch 112/500, Loss: 0.5855\n","Epoch 113/500, Loss: 0.5852\n","Epoch 114/500, Loss: 0.5850\n","Epoch 115/500, Loss: 0.5850\n","Epoch 116/500, Loss: 0.5845\n","Epoch 117/500, Loss: 0.5843\n","Epoch 118/500, Loss: 0.5842\n","Epoch 119/500, Loss: 0.5838\n","Epoch 120/500, Loss: 0.5835\n","Epoch 121/500, Loss: 0.5836\n","Epoch 122/500, Loss: 0.5832\n","Epoch 123/500, Loss: 0.5830\n","Epoch 124/500, Loss: 0.5829\n","Epoch 125/500, Loss: 0.5827\n","Epoch 126/500, Loss: 0.5826\n","Epoch 127/500, Loss: 0.5824\n","Epoch 128/500, Loss: 0.5819\n","Epoch 129/500, Loss: 0.5818\n","Epoch 130/500, Loss: 0.5818\n","Epoch 131/500, Loss: 0.5814\n","Epoch 132/500, Loss: 0.5812\n","Epoch 133/500, Loss: 0.5810\n","Epoch 134/500, Loss: 0.5805\n","Epoch 135/500, Loss: 0.5807\n","Epoch 136/500, Loss: 0.5801\n","Epoch 137/500, Loss: 0.5801\n","Epoch 138/500, Loss: 0.5797\n","Epoch 139/500, Loss: 0.5796\n","Epoch 140/500, Loss: 0.5795\n","Epoch 141/500, Loss: 0.5795\n","Epoch 142/500, Loss: 0.5792\n","Epoch 143/500, Loss: 0.5790\n","Epoch 144/500, Loss: 0.5788\n","Epoch 145/500, Loss: 0.5785\n","Epoch 146/500, Loss: 0.5782\n","Epoch 147/500, Loss: 0.5783\n","Epoch 148/500, Loss: 0.5779\n","Epoch 149/500, Loss: 0.5778\n","Epoch 150/500, Loss: 0.5777\n","Epoch 151/500, Loss: 0.5775\n","Epoch 152/500, Loss: 0.5775\n","Epoch 153/500, Loss: 0.5769\n","Epoch 154/500, Loss: 0.5769\n","Epoch 155/500, Loss: 0.5766\n","Epoch 156/500, Loss: 0.5767\n","Epoch 157/500, Loss: 0.5764\n","Epoch 158/500, Loss: 0.5762\n","Epoch 159/500, Loss: 0.5758\n","Epoch 160/500, Loss: 0.5757\n","Epoch 161/500, Loss: 0.5757\n","Epoch 162/500, Loss: 0.5756\n","Epoch 163/500, Loss: 0.5755\n","Epoch 164/500, Loss: 0.5751\n","Epoch 165/500, Loss: 0.5749\n","Epoch 166/500, Loss: 0.5746\n","Epoch 167/500, Loss: 0.5746\n","Epoch 168/500, Loss: 0.5747\n","Epoch 169/500, Loss: 0.5746\n","Epoch 170/500, Loss: 0.5741\n","Epoch 171/500, Loss: 0.5740\n","Epoch 172/500, Loss: 0.5738\n","Epoch 173/500, Loss: 0.5736\n","Epoch 174/500, Loss: 0.5737\n","Epoch 175/500, Loss: 0.5734\n","Epoch 176/500, Loss: 0.5734\n","Epoch 177/500, Loss: 0.5731\n","Epoch 178/500, Loss: 0.5732\n","Epoch 179/500, Loss: 0.5727\n","Epoch 180/500, Loss: 0.5726\n","Epoch 181/500, Loss: 0.5727\n","Epoch 182/500, Loss: 0.5722\n","Epoch 183/500, Loss: 0.5721\n","Epoch 184/500, Loss: 0.5721\n","Epoch 185/500, Loss: 0.5716\n","Epoch 186/500, Loss: 0.5717\n","Epoch 187/500, Loss: 0.5714\n","Epoch 188/500, Loss: 0.5715\n","Epoch 189/500, Loss: 0.5710\n","Epoch 190/500, Loss: 0.5712\n","Epoch 191/500, Loss: 0.5708\n","Epoch 192/500, Loss: 0.5706\n","Epoch 193/500, Loss: 0.5706\n","Epoch 194/500, Loss: 0.5704\n","Epoch 195/500, Loss: 0.5702\n","Epoch 196/500, Loss: 0.5701\n","Epoch 197/500, Loss: 0.5699\n","Epoch 198/500, Loss: 0.5699\n","Epoch 199/500, Loss: 0.5695\n","Epoch 200/500, Loss: 0.5694\n","Epoch 201/500, Loss: 0.5694\n","Epoch 202/500, Loss: 0.5692\n","Epoch 203/500, Loss: 0.5692\n","Epoch 204/500, Loss: 0.5690\n","Epoch 205/500, Loss: 0.5687\n","Epoch 206/500, Loss: 0.5686\n","Epoch 207/500, Loss: 0.5685\n","Epoch 208/500, Loss: 0.5683\n","Epoch 209/500, Loss: 0.5683\n","Epoch 210/500, Loss: 0.5680\n","Epoch 211/500, Loss: 0.5680\n","Epoch 212/500, Loss: 0.5677\n","Epoch 213/500, Loss: 0.5676\n","Epoch 214/500, Loss: 0.5676\n","Epoch 215/500, Loss: 0.5675\n","Epoch 216/500, Loss: 0.5672\n","Epoch 217/500, Loss: 0.5670\n","Epoch 218/500, Loss: 0.5669\n","Epoch 219/500, Loss: 0.5670\n","Epoch 220/500, Loss: 0.5667\n","Epoch 221/500, Loss: 0.5668\n","Epoch 222/500, Loss: 0.5663\n","Epoch 223/500, Loss: 0.5663\n","Epoch 224/500, Loss: 0.5660\n","Epoch 225/500, Loss: 0.5659\n","Epoch 226/500, Loss: 0.5657\n","Epoch 227/500, Loss: 0.5659\n","Epoch 228/500, Loss: 0.5656\n","Epoch 229/500, Loss: 0.5652\n","Epoch 230/500, Loss: 0.5652\n","Epoch 231/500, Loss: 0.5653\n","Epoch 232/500, Loss: 0.5650\n","Epoch 233/500, Loss: 0.5650\n","Epoch 234/500, Loss: 0.5647\n","Epoch 235/500, Loss: 0.5646\n","Epoch 236/500, Loss: 0.5646\n","Epoch 237/500, Loss: 0.5642\n","Epoch 238/500, Loss: 0.5643\n","Epoch 239/500, Loss: 0.5640\n","Epoch 240/500, Loss: 0.5639\n","Epoch 241/500, Loss: 0.5638\n","Epoch 242/500, Loss: 0.5637\n","Epoch 243/500, Loss: 0.5632\n","Epoch 244/500, Loss: 0.5635\n","Epoch 245/500, Loss: 0.5632\n","Epoch 246/500, Loss: 0.5631\n","Epoch 247/500, Loss: 0.5628\n","Epoch 248/500, Loss: 0.5627\n","Epoch 249/500, Loss: 0.5625\n","Epoch 250/500, Loss: 0.5623\n","Epoch 251/500, Loss: 0.5625\n","Epoch 252/500, Loss: 0.5624\n","Epoch 253/500, Loss: 0.5621\n","Epoch 254/500, Loss: 0.5620\n","Epoch 255/500, Loss: 0.5618\n","Epoch 256/500, Loss: 0.5615\n","Epoch 257/500, Loss: 0.5615\n","Epoch 258/500, Loss: 0.5614\n","Epoch 259/500, Loss: 0.5617\n","Epoch 260/500, Loss: 0.5616\n","Epoch 261/500, Loss: 0.5611\n","Epoch 262/500, Loss: 0.5611\n","Epoch 263/500, Loss: 0.5607\n","Epoch 264/500, Loss: 0.5609\n","Epoch 265/500, Loss: 0.5605\n","Epoch 266/500, Loss: 0.5604\n","Epoch 267/500, Loss: 0.5605\n","Epoch 268/500, Loss: 0.5603\n","Epoch 269/500, Loss: 0.5603\n","Epoch 270/500, Loss: 0.5603\n","Epoch 271/500, Loss: 0.5598\n","Epoch 272/500, Loss: 0.5600\n","Epoch 273/500, Loss: 0.5594\n","Epoch 274/500, Loss: 0.5594\n","Epoch 275/500, Loss: 0.5592\n","Epoch 276/500, Loss: 0.5593\n","Epoch 277/500, Loss: 0.5591\n","Epoch 278/500, Loss: 0.5591\n","Epoch 279/500, Loss: 0.5589\n","Epoch 280/500, Loss: 0.5587\n","Epoch 281/500, Loss: 0.5585\n","Epoch 282/500, Loss: 0.5584\n","Epoch 283/500, Loss: 0.5586\n","Epoch 284/500, Loss: 0.5583\n","Epoch 285/500, Loss: 0.5580\n","Epoch 286/500, Loss: 0.5578\n","Epoch 287/500, Loss: 0.5577\n","Epoch 288/500, Loss: 0.5577\n","Epoch 289/500, Loss: 0.5575\n","Epoch 290/500, Loss: 0.5576\n","Epoch 291/500, Loss: 0.5572\n","Epoch 292/500, Loss: 0.5571\n","Epoch 293/500, Loss: 0.5572\n","Epoch 294/500, Loss: 0.5569\n","Epoch 295/500, Loss: 0.5568\n","Epoch 296/500, Loss: 0.5568\n","Epoch 297/500, Loss: 0.5564\n","Epoch 298/500, Loss: 0.5566\n","Epoch 299/500, Loss: 0.5563\n","Epoch 300/500, Loss: 0.5562\n","Epoch 301/500, Loss: 0.5562\n","Epoch 302/500, Loss: 0.5557\n","Epoch 303/500, Loss: 0.5560\n","Epoch 304/500, Loss: 0.5558\n","Epoch 305/500, Loss: 0.5557\n","Epoch 306/500, Loss: 0.5555\n","Epoch 307/500, Loss: 0.5552\n","Epoch 308/500, Loss: 0.5553\n","Epoch 309/500, Loss: 0.5554\n","Epoch 310/500, Loss: 0.5549\n","Epoch 311/500, Loss: 0.5548\n","Epoch 312/500, Loss: 0.5548\n","Epoch 313/500, Loss: 0.5546\n","Epoch 314/500, Loss: 0.5544\n","Epoch 315/500, Loss: 0.5544\n","Epoch 316/500, Loss: 0.5543\n","Epoch 317/500, Loss: 0.5544\n","Epoch 318/500, Loss: 0.5541\n","Epoch 319/500, Loss: 0.5541\n","Epoch 320/500, Loss: 0.5538\n","Epoch 321/500, Loss: 0.5539\n","Epoch 322/500, Loss: 0.5537\n","Epoch 323/500, Loss: 0.5536\n","Epoch 324/500, Loss: 0.5533\n","Epoch 325/500, Loss: 0.5532\n","Epoch 326/500, Loss: 0.5530\n","Epoch 327/500, Loss: 0.5529\n","Epoch 328/500, Loss: 0.5529\n","Epoch 329/500, Loss: 0.5529\n","Epoch 330/500, Loss: 0.5525\n","Epoch 331/500, Loss: 0.5528\n","Epoch 332/500, Loss: 0.5524\n","Epoch 333/500, Loss: 0.5523\n","Epoch 334/500, Loss: 0.5524\n","Epoch 335/500, Loss: 0.5521\n","Epoch 336/500, Loss: 0.5518\n","Epoch 337/500, Loss: 0.5519\n","Epoch 338/500, Loss: 0.5519\n","Epoch 339/500, Loss: 0.5515\n","Epoch 340/500, Loss: 0.5514\n","Epoch 341/500, Loss: 0.5514\n","Epoch 342/500, Loss: 0.5515\n","Epoch 343/500, Loss: 0.5512\n","Epoch 344/500, Loss: 0.5510\n","Epoch 345/500, Loss: 0.5512\n","Epoch 346/500, Loss: 0.5509\n","Epoch 347/500, Loss: 0.5509\n","Epoch 348/500, Loss: 0.5506\n","Epoch 349/500, Loss: 0.5505\n","Epoch 350/500, Loss: 0.5505\n","Epoch 351/500, Loss: 0.5501\n","Epoch 352/500, Loss: 0.5502\n","Epoch 353/500, Loss: 0.5499\n","Epoch 354/500, Loss: 0.5499\n","Epoch 355/500, Loss: 0.5499\n","Epoch 356/500, Loss: 0.5496\n","Epoch 357/500, Loss: 0.5494\n","Epoch 358/500, Loss: 0.5496\n","Epoch 359/500, Loss: 0.5494\n","Epoch 360/500, Loss: 0.5492\n","Epoch 361/500, Loss: 0.5491\n","Epoch 362/500, Loss: 0.5489\n","Epoch 363/500, Loss: 0.5489\n","Epoch 364/500, Loss: 0.5489\n","Epoch 365/500, Loss: 0.5488\n","Epoch 366/500, Loss: 0.5485\n","Epoch 367/500, Loss: 0.5485\n","Epoch 368/500, Loss: 0.5483\n","Epoch 369/500, Loss: 0.5480\n","Epoch 370/500, Loss: 0.5481\n","Epoch 371/500, Loss: 0.5480\n","Epoch 372/500, Loss: 0.5478\n","Epoch 373/500, Loss: 0.5479\n","Epoch 374/500, Loss: 0.5479\n","Epoch 375/500, Loss: 0.5477\n","Epoch 376/500, Loss: 0.5475\n","Epoch 377/500, Loss: 0.5473\n","Epoch 378/500, Loss: 0.5473\n","Epoch 379/500, Loss: 0.5471\n","Epoch 380/500, Loss: 0.5472\n","Epoch 381/500, Loss: 0.5468\n","Epoch 382/500, Loss: 0.5470\n","Epoch 383/500, Loss: 0.5469\n","Epoch 384/500, Loss: 0.5465\n","Epoch 385/500, Loss: 0.5465\n","Epoch 386/500, Loss: 0.5462\n","Epoch 387/500, Loss: 0.5464\n","Epoch 388/500, Loss: 0.5462\n","Epoch 389/500, Loss: 0.5461\n","Epoch 390/500, Loss: 0.5457\n","Epoch 391/500, Loss: 0.5459\n","Epoch 392/500, Loss: 0.5457\n","Epoch 393/500, Loss: 0.5457\n","Epoch 394/500, Loss: 0.5456\n","Epoch 395/500, Loss: 0.5456\n","Epoch 396/500, Loss: 0.5452\n","Epoch 397/500, Loss: 0.5452\n","Epoch 398/500, Loss: 0.5451\n","Epoch 399/500, Loss: 0.5450\n","Epoch 400/500, Loss: 0.5448\n","Epoch 401/500, Loss: 0.5447\n","Epoch 402/500, Loss: 0.5447\n","Epoch 403/500, Loss: 0.5445\n","Epoch 404/500, Loss: 0.5444\n","Epoch 405/500, Loss: 0.5444\n","Epoch 406/500, Loss: 0.5442\n","Epoch 407/500, Loss: 0.5441\n","Epoch 408/500, Loss: 0.5441\n","Epoch 409/500, Loss: 0.5440\n","Epoch 410/500, Loss: 0.5439\n","Epoch 411/500, Loss: 0.5437\n","Epoch 412/500, Loss: 0.5437\n","Epoch 413/500, Loss: 0.5433\n","Epoch 414/500, Loss: 0.5435\n","Epoch 415/500, Loss: 0.5432\n","Epoch 416/500, Loss: 0.5432\n","Epoch 417/500, Loss: 0.5431\n","Epoch 418/500, Loss: 0.5432\n","Epoch 419/500, Loss: 0.5428\n","Epoch 420/500, Loss: 0.5428\n","Epoch 421/500, Loss: 0.5425\n","Epoch 422/500, Loss: 0.5428\n","Epoch 423/500, Loss: 0.5422\n","Epoch 424/500, Loss: 0.5423\n","Epoch 425/500, Loss: 0.5424\n","Epoch 426/500, Loss: 0.5420\n","Epoch 427/500, Loss: 0.5419\n","Epoch 428/500, Loss: 0.5419\n","Epoch 429/500, Loss: 0.5418\n","Epoch 430/500, Loss: 0.5416\n","Epoch 431/500, Loss: 0.5416\n","Epoch 432/500, Loss: 0.5414\n","Epoch 433/500, Loss: 0.5414\n","Epoch 434/500, Loss: 0.5412\n","Epoch 435/500, Loss: 0.5411\n","Epoch 436/500, Loss: 0.5410\n","Epoch 437/500, Loss: 0.5410\n","Epoch 438/500, Loss: 0.5408\n","Epoch 439/500, Loss: 0.5408\n","Epoch 440/500, Loss: 0.5405\n","Epoch 441/500, Loss: 0.5405\n","Epoch 442/500, Loss: 0.5405\n","Epoch 443/500, Loss: 0.5403\n","Epoch 444/500, Loss: 0.5403\n","Epoch 445/500, Loss: 0.5400\n","Epoch 446/500, Loss: 0.5399\n","Epoch 447/500, Loss: 0.5400\n","Epoch 448/500, Loss: 0.5400\n","Epoch 449/500, Loss: 0.5398\n","Epoch 450/500, Loss: 0.5397\n","Epoch 451/500, Loss: 0.5396\n","Epoch 452/500, Loss: 0.5395\n","Epoch 453/500, Loss: 0.5395\n","Epoch 454/500, Loss: 0.5395\n","Epoch 455/500, Loss: 0.5390\n","Epoch 456/500, Loss: 0.5390\n","Epoch 457/500, Loss: 0.5388\n","Epoch 458/500, Loss: 0.5390\n","Epoch 459/500, Loss: 0.5387\n","Epoch 460/500, Loss: 0.5387\n","Epoch 461/500, Loss: 0.5385\n","Epoch 462/500, Loss: 0.5382\n","Epoch 463/500, Loss: 0.5383\n","Epoch 464/500, Loss: 0.5381\n","Epoch 465/500, Loss: 0.5382\n","Epoch 466/500, Loss: 0.5379\n","Epoch 467/500, Loss: 0.5378\n","Epoch 468/500, Loss: 0.5378\n","Epoch 469/500, Loss: 0.5377\n","Epoch 470/500, Loss: 0.5374\n","Epoch 471/500, Loss: 0.5375\n","Epoch 472/500, Loss: 0.5373\n","Epoch 473/500, Loss: 0.5373\n","Epoch 474/500, Loss: 0.5371\n","Epoch 475/500, Loss: 0.5369\n","Epoch 476/500, Loss: 0.5367\n","Epoch 477/500, Loss: 0.5370\n","Epoch 478/500, Loss: 0.5368\n","Epoch 479/500, Loss: 0.5365\n","Epoch 480/500, Loss: 0.5365\n","Epoch 481/500, Loss: 0.5365\n","Epoch 482/500, Loss: 0.5366\n","Epoch 483/500, Loss: 0.5362\n","Epoch 484/500, Loss: 0.5362\n","Epoch 485/500, Loss: 0.5359\n","Epoch 486/500, Loss: 0.5359\n","Epoch 487/500, Loss: 0.5360\n","Epoch 488/500, Loss: 0.5356\n","Epoch 489/500, Loss: 0.5356\n","Epoch 490/500, Loss: 0.5354\n","Epoch 491/500, Loss: 0.5355\n","Epoch 492/500, Loss: 0.5352\n","Epoch 493/500, Loss: 0.5353\n","Epoch 494/500, Loss: 0.5352\n","Epoch 495/500, Loss: 0.5348\n","Epoch 496/500, Loss: 0.5349\n","Epoch 497/500, Loss: 0.5347\n","Epoch 498/500, Loss: 0.5349\n","Epoch 499/500, Loss: 0.5345\n","Epoch 500/500, Loss: 0.5345\n"]}]},{"cell_type":"code","source":["# Evaluation mode\n","model.eval()\n","accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes) #initialize accuracy\n","\n","with torch.no_grad(): # Disable gradient calculations for evaluation\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        predicted = predicted.long()\n","        accuracy_metric.update(predicted, labels)\n","\n","# Calculate evaluation metrics\n","accuracy = accuracy_metric.compute()\n","\n","print(f\"Test Accuracy: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Enb28TkDH8uG","executionInfo":{"status":"ok","timestamp":1736431300239,"user_tz":-120,"elapsed":247,"user":{"displayName":"Alex Nielsen","userId":"13445153291848091451"}},"outputId":"f2b6d96a-7e52-4a6c-a172-811c6616ce10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7292\n"]}]}]}